# -*- coding: utf-8 -*-
"""Project_Credit_Risk_Intership_IDX_Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TOnDFE9BxH-CgxUXd2mfX5a9Qno011B-

## Preprosessing
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from scipy.stats import gaussian_kde
import urllib.request
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import *
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from keras.callbacks import EarlyStopping
import tensorflow as tf
from numpy import array
from google.colab import drive
import seaborn as sns
import numpy
from tensorflow.keras.optimizers import Adam
import plotly.express as px
import plotly.graph_objects as go

# mengunduh file csv dari URL dan menyimpannya sebagai objek response
url = "https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/idx-partners/loan_data_2007_2014.csv"
response = urllib.request.urlopen(url)

# membaca data csv sebagai dataframe menggunakan pandas
df = pd.read_csv(response)
# menampilkan dataframe

# menampilkan dataframe
pd.set_option('display.max_columns', 90)

def null_percentage(df):
    total = df.isnull().sum().sort_values(ascending = False)
    total = total[total !=0]
    percent = round(100 * total/ len(df),2)
    return pd.concat([total, percent], axis = 1, keys = ['Total Null' , 'Percent'])

null_percentage(df)

pd.set_option('display.max_columns', 90)
data = df[['id', 'loan_amnt', 'int_rate',
         'term','installment', 'grade', 'sub_grade', 'emp_title', 
         'emp_length','home_ownership','annual_inc','purpose',
         'issue_d','loan_status','dti', 'delinq_2yrs','earliest_cr_line', 'open_acc','revol_util', 'collections_12_mths_ex_med',
        'total_acc','total_pymnt','revol_bal', 'tot_coll_amt','inq_last_6mths', 'pub_rec','acc_now_delinq',
         'last_pymnt_d','last_credit_pull_d','tot_cur_bal','total_rev_hi_lim']]
         
drop_col = ['revol_util', 'issue_d', 'delinq_2yrs', 'delinq_2yrs','earliest_cr_line','open_acc','collections_12_mths_ex_med','total_acc', 'total_pymnt',
            'revol_bal','tot_coll_amt','inq_last_6mths','pub_rec','acc_now_delinq','last_pymnt_d','last_credit_pull_d','tot_cur_bal','total_rev_hi_lim']

for col in drop_col :
    if col in data.columns:
        data = data.drop(col, axis = 1)

data

data_loan = data.groupby('loan_status')['loan_status'].count().to_frame()
data_loan = data_loan.rename(columns = {'loan_status' : 'count'})
data_loan = data_loan.sort_values('count', ascending = False)
data_loan

plot_loan = px.bar(data_loan, x = data_loan.index, y = 'count')
plot_loan.update_layout(title = 'Loan Status',width = 1500, height = 600 )
plot_loan.update_xaxes(tickangle = 90)
plot_loan.show()

data_loan = data_loan.reset_index()
data_loan = data_loan.sort_values('count', ascending = False)
data_loan_name = data_loan.iloc[0:3]["loan_status"]
data = data[data['loan_status'].isin(data_loan_name)]
data.drop(data[data['loan_status'] == 'Current'].index, inplace = True)

data = data[data['loan_status'].isin(data_loan_name)]
data.drop(data[data['loan_status'] == 'Current'].index, inplace = True)

plt.figure(figsize= (15,7))
sns.set_context('paper', font_scale = 1)

sns.heatmap(data.assign(home_ownership = data.home_ownership.astype('category').cat.codes,
                        purpose = data.purpose.astype('category').cat.codes,
                        term = data.term.astype('category').cat.codes,
                        grade = data.grade.astype('category').cat.codes,
                        sub_grade = data.sub_grade.astype('category').cat.codes,
                        emp_title = data.emp_title.astype('category').cat.codes,
                        emp_length = data.emp_length.astype('category').cat.codes,
                        loan_status = data.loan_status.astype('category').cat.codes).corr(),
                        annot = True, cmap = 'RdYlGn', vmin = -1, vmax = 1, linewidths = 0.5)

plt.show

data = data.drop(['id', 'grade', 'sub_grade', 'installment'], axis = 1)
data = data.dropna()

data

bins = [-np.inf, 1, 5, 9, np.inf]
labels = ['<1 year', '1-5 years', '6-9 years', '10+ years']

data['emp_length']=  pd.cut(pd.to_numeric(data['emp_length'].str.extract('(\d+)')[0]), bins = bins, labels = labels)

def plot_his_dis(data):
    kde = gaussian_kde(data)
    x = np.linspace(data.min(), data.max())
    y = kde(x)

    plot_his = px.histogram(x = data, nbins = 50, opacity = 0.5)

    plot_his.add_trace(go.Scatter(x = x,
                                  y = y,
                                  name = 'Density',
                                  yaxis = 'y2',
                                  line = dict(color = 'red', width = 3)))
    plot_his.update_layout(bargroupgap = 0.2)
    plot_his.update_layout(yaxis2 = dict(title = 'Density', 
                                         title_font = dict(family = 'arial'),
                                         overlaying = 'y', side = 'right'))
    return plot_his

plot_his_dis(data['loan_amnt'])

plot_his_dis(data['dti'])

plot_his_dis(data['int_rate'])

def boxplot(data):
    sns.set(style = 'whitegrid')
    plt.figure(figsize = (10,6))
    sns.boxplot(x=data)
    plt.xlabel('Annual Income')
    return plt

boxplot(data['annual_inc'])

data_term = data.groupby('term')['term'].count().sort_values(ascending = False)
data_emp_length = data.groupby('emp_length')['emp_length'].count().sort_values(ascending= False)
data_home_ownership = data.groupby('home_ownership')['home_ownership'].count().sort_values(ascending = False)
data_purpose = data.groupby('purpose')['purpose'].count().sort_values(ascending = False)

plot_home_ownership = px.bar(data_home_ownership, x = data_home_ownership.index, y = data_home_ownership, color =data_home_ownership.index )
plot_home_ownership.update_xaxes(title ='Home Owner')
plot_home_ownership.update_yaxes(title = 'Jumlah')
plot_home_ownership.show()

data_name_home_owner = ['MORTGAGE', 'RENT', 'OWN']
data = data[data['home_ownership'].isin(data_name_home_owner)]
data['home_ownership'].unique()

plot_purpose = px.bar(data_purpose, x = data_purpose.index, y = data_purpose, color = data_purpose.index)
plot_purpose.update_xaxes(title = 'Purpose')
plot_purpose.update_yaxes(title = 'Jumlah')
plot_purpose.show()

plot_emp_length = px.bar(data_emp_length, x =data_emp_length.index, y = data_emp_length, color = data_emp_length.index)
plot_emp_length.update_xaxes(title = 'Employee length')
plot_emp_length.update_yaxes(title = 'Jumlah')
plot_emp_length.show()

plot_term = px.bar(data_term, x = data_term.index, y = data_term, color = data_term.index)
plot_term.update_xaxes(title = 'Term')
plot_term.update_yaxes(title = 'Jumlah')
plot_term.show()

data_dist_loan = data.groupby(['annual_inc', 'term', 'loan_status'])['loan_amnt'].mean().reset_index()
data_dist_loan

fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (15,7))

sns.boxplot(data = data_dist_loan, x = 'loan_amnt', y = 'term', ax = axes[0])
sns.kdeplot(data = data_dist_loan, x = 'loan_amnt',hue = 'term',ax = axes[1])


axes[0].set(xticks = [], yticks = [])
axes[0].set_xlabel(None)
axes[0].set_ylabel(None)




plt.show()

data_dist_loan

data_dist_loan['loan_status_term'] = data_dist_loan['term'] + " " + data_dist_loan['loan_status']
data_dist_loan

fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (15,10))

sns.boxplot(data = data_dist_loan, x = 'loan_amnt', y = 'loan_status_term', ax = axes[0])
sns.kdeplot(data = data_dist_loan, x = 'loan_amnt',hue = 'loan_status_term',ax = axes[1])


axes[0].set(xticks = [], yticks = [])
axes[0].set_xlabel(None)
axes[0].set_ylabel(None)


plt.show()

fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (15,7))

sns.boxplot(data = data, x = 'int_rate', y = 'home_ownership', ax = axes[0])
sns.kdeplot(data = data, x = 'int_rate',hue = 'home_ownership',ax = axes[1])


axes[0].set(xticks = [], yticks = [])
axes[0].set_xlabel(None)
axes[0].set_ylabel(None)




plt.show()

data = data.drop(['int_rate', 'dti', 'emp_title'], axis = 1)
data_preprocessing = data



"""###Encoding and normalization"""

# encode kategorikal features
categorical_features = ['term',  'emp_length', 'home_ownership', 'purpose', 'loan_status']

for col in categorical_features:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])


numerical_features = ['loan_amnt', 'annual_inc']


for col in numerical_features:
  scaler = MinMaxScaler()
  data[col] = scaler.fit_transform(data[[col]])

# split dataset into train and test set
X = data.drop('loan_status', axis=1)
y = data['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Imbelance Data"""

data_loan_status = data.groupby('loan_status').size().reset_index(name='counts')
data_loan_status

plot_imbalance = px.bar(data_loan_status, x = 'loan_status', y = 'counts')
plot_imbalance.show()

# Lakukan SMOTE pada subset training
# Lakukan Random Oversampling pada subset training
ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

"""## Random Forest """

# create random forest classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train_resampled, y_train_resampled)

scores = cross_val_score(rf_clf, X_train_resampled, y_train_resampled, cv=5)
print("Cross-validation scores:", scores)
print("Mean score:", scores.mean())

y_pred = rf_clf.predict(X_test)

# generate classification report with confusion matrix
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# menghitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

# menampilkan confusion matrix dalam bentuk heatmap
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

joblib.dump(rf_clf, '/media/model.pkl')

"""## SVM

"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Train SVM Classifier
svm = SVC(random_state=42)
svm.fit(X_train_resampled, y_train_resampled)

# Predict on test data
y_pred = svm.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)

# generate classification report with confusion matrix
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# save the trained model for deployment
joblib.dump(rf_clf, '/media/model_svm.pkl')

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# save the trained model for deployment
joblib.dump(rf_clf, '/media/model_svm.pkl')

# melakukan prediksi pada data testing
y_pred = rf_clf.predict(X_test)

# menghitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

# menampilkan confusion matrix dalam bentuk heatmap
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

parameters = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}

svc = SVC()
clf = GridSearchCV(svc, parameters)
clf.fit(X_train_resampled, y_train_resampled)

# mencetak parameter terbaik
print(clf.best_params_)

from sklearn.metrics import accuracy_score

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Akurasi model: {:.2f}%".format(accuracy * 100))

import joblib

joblib.dump(clf, 'credit_risk_model_svm.pkl')

"""## Decision Tree

"""

from sklearn.metrics import accuracy_score
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train_resampled, y_train_resampled)

# Predict on test data
y_pred = dt.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Decision Tree Classifier accuracy:", accuracy)

# generate classification report with confusion matrix
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""## LSTM

"""

X = data.drop('loan_status', axis = 1)
y = data['loan_status']

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

train_size = int(len(X_resampled) *0.8)
test_size = int(len(X_resampled) - train_size)
print(train_size)
print(test_size)

y_test.shape

X_train_resampled =  X_resampled.iloc[:train_size,:]
X_test_resampled = X_resampled.iloc[train_size:,:]
y_train_resampled= y_resampled.iloc[:train_size]
y_test_resampled = y_resampled.iloc[train_size:]

print(f'X train : {X_train}, dengan Shape {X_train.shape}')
print(f'y train : {y_train}, dengan Shape {y_train.shape}')
print(f'X test : {X_test}, dengan Shape {X_test.shape}')
print(f'y test : {y_test}, dengan Shape {X_test.shape}')

def create_data(dataset, timestep):

    data = []
    for i in range(len(dataset) - timestep):
        data.append(dataset[i:i+timestep])
    return np.array(data)

timestep = 365


X_train_resampled = create_data(X_train_resampled, timestep)
y_resampled = create_data(y_resampled, timestep)
X_test_resampled = create_data(X_test_resampled, timestep)
y_test_resampled = create_data(y_test_resampled, timestep)

print(f"X_train Shape : {X_train.shape}")
print(f'y_train Shape : {y_train.shape}')
print(f'X_test Shape : {X_test.shape}')
print(f'y_test Shape : {y_test.shape}')

X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])
y_test = y_test.reshape(y_test.shape[0], y_test.shape[1])

#from tensorflow.keras.callbacks import EarlyStopping
model=Sequential()
model.add(LSTM(32,return_sequences=True,input_shape=(100,6)))
model.add(Dropout(0.2))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128,return_sequences =True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dense(1))



#model.compile(loss='mean_absolute_error',optimizer= Adam(lr=0.000001))
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])
#early_stopping = EarlyStopping(monitor='val_loss', patience=5)
history = model.fit(X_train, y_train, epochs=10, batch_size=320, validation_data=(X_test, y_test), shuffle=False)

model.save('lstm_model.h5')

"""## Decision Tree Hyperparams"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import pandas as pd

# Load data


# Split data menjadi train dan test set
X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled, test_size=0.2)

# Definisikan hyperparameter grid untuk tuning
param_grid = {'max_depth': [20, 30, 40, None], 
              'max_features': ['sqrt', 'log2','auto', None],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4,6],
              'splitter' : ['best', 'random'],
              'criterion': ['gini', 'entropy', 'log_loss']}

# Buat model Decision Tree
dt = DecisionTreeClassifier()

# Buat objek GridSearchCV untuk tuning hyperparameter
grid = GridSearchCV(dt, param_grid=param_grid, cv=5, n_jobs=-1)

# Train model dengan GridSearchCV
grid.fit(X_train, y_train)

# Cetak hasil tuning
print("Best Parameters:\n", grid.best_params_)
print("\nBest Estimator:\n", grid.best_estimator_)

# Prediksi hasil test set
y_pred = grid.predict(X_test)

# Evaluasi performa model
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""##ANN"""

# Import library yang dibutuhkan
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout


# Split dataset menjadi training set dan testing set
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)


model = Sequential()
model.add(Dense(1536, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(.3))
model.add(Dense(512, activation='relu'))
model.add(Dropout(.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(.3))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=100, batch_size=62, validation_data=(X_test, y_test))

# Define the function to create the model
def create_model(neurons=1, dropout_rate=0.0, optimizer='adam'):
    # create model
    model = Sequential()
    model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model



# create the Keras model
model = KerasClassifier(build_fn=create_model)

# define the grid search parameters
param_grid = {'neurons': [8, 16, 32],
              'dropout_rate': [0.1, 0.2, 0.3],
              'optimizer': ['adam', 'rmsprop']}

# create the grid search object
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')

# fit the grid search object to the data
grid_result = grid.fit(X_train, y_train)

# print the results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

model.save("my_model")

"""## KNN

"""

from sklearn.neighbors import KNeighborsClassifier

# inisialisasi model KNN
knn = KNeighborsClassifier()

# definisikan hyperparameter yang akan diuji
params = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'p': [1, 2]}

# inisialisasi GridSearchCV untuk melakukan tuning hyperparameter
grid_search = GridSearchCV(knn, params, cv=5, n_jobs=-1)

# pelatihan model KNN dengan GridSearchCV
grid_search.fit(X_train_resampled, y_train_resampled)

# evaluasi model KNN dengan hyperparameter tuning pada data uji
accuracy = grid_search.score(X_test, y_test)

# print hyperparameter terbaik dan akurasi model
print("Hyperparameter terbaik:", grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

from joblib import dump
dump(grid_search, 'knn_model.joblib')

grid_search.predict()

# Predict on test data
y_pred = grid_search.predict(X_test)

# Evaluate accuracy


# generate classification report with confusion matrix
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))