# -*- coding: utf-8 -*-
"""Credit Risk Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdzhubOgRkQDec57DqNt6RGjwbC0sR9v

# Preprocessing

## Import library and load data
"""

#from google.colab import drive 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns 
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import gaussian_kde
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import joblib
from sklearn.svm import SVC
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.neighbors import KNeighborsClassifier
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.ensemble import RandomForestClassifier
from joblib import dump

#drive.mount('/content/drive')

#ls

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Data Credit Risk

dataset = pd.read_csv('credit_risk_dataset.csv')
dataset

"""## features Selection """

plt.figure(figsize = (15,7))

sns.heatmap(dataset.assign(person_home_ownership = dataset.person_home_ownership.astype('category').cat.codes,
                           loan_intent = dataset.loan_intent.astype('category').cat.codes,
                           loan_grade = dataset.loan_grade.astype('category').cat.codes,
                           cb_person_default_on_file = dataset.cb_person_default_on_file.astype('category').cat.codes).corr(),
            annot = True, cmap ='RdYlGn', vmin = -1, vmax = 1, linewidths = 0.5)


"""Drop features"""

features_drop = ['cb_person_cred_hist_length', 'cb_person_default_on_file','loan_percent_income', 'loan_int_rate',]

for col in features_drop:
  dataset = dataset.drop(col,axis=1)

"""dropna Values and unknow data"""

dataset = dataset.dropna()
dataset = dataset.drop(dataset[dataset['person_emp_length'] == 123].index)
dataset = dataset.drop(dataset[(dataset['person_age'] == 144) | (dataset['person_age'] == 123)].index)



"""# Machine Learning

## Outlier
"""

dataset

def plot_boxplot(dataset):
  column_outlier = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt']

  for col in column_outlier:
    sns.set(style = 'whitegrid')
    plt.figure(figsize = (10,6))
    sns.boxplot(x=dataset[col])
    plt.title(f'Boxplot Outlier {col}')
    

def remove_outlier(dataset):
  column_outlier = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt']

  for col in column_outlier:
    Q1 = dataset[col].quantile(0.25)
    Q3 = dataset[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - (1.5 * IQR)
    upper_bound = Q3 + (0.4 * IQR)

    dataset = dataset[(dataset[col] > lower_bound) & (dataset[col] < upper_bound)]

    return dataset

plot_boxplot(dataset)

dataset = remove_outlier(dataset)

"""## Encode and Normalization"""

dataset['loan_grade']  = dataset['loan_grade'].replace(['D','E','F','G'], 'D')
dataset

dataset = dataset.dropna()

dataset = dataset.sort_values('loan_grade')
# encode kategorikal features
categorical_features = ['person_home_ownership', 'loan_intent', 'loan_grade']
# person_home_ownership = {rent : 3, own : 2, mortgage : 0, other : 1}
# loan_intet = {Debtconsolidation : 0, Venture : 5, Medical : 3, Education : 1, Homeimprovement : 2, Personal : 4}
#loan_grade = {A : 0, B : 1, C : 2, D : 3, E : 4, F : 5, G : 6}

for col in categorical_features:
  le = LabelEncoder()
  dataset[col] = le.fit_transform(dataset[col])

numerical_features = ['person_age', 'person_income', 'person_emp_length','loan_amnt']

for col in numerical_features:
  scaler  = MinMaxScaler()
  dataset[col] = scaler.fit_transform(dataset[[col]])

dataset

"""## Train test Split"""

# split dataset into train and test set
X = dataset.drop(['loan_status', 'loan_grade'], axis=1)
y_loan_status = dataset['loan_status']
y_loan_grade = dataset['loan_grade']
X_train, X_test, y_train_loan_status, y_test_loan_status = train_test_split(X, y_loan_status, test_size=0.2, random_state=42)
X_train, X_test, y_train_loan_grade, y_test_loan_grade = train_test_split(X, y_loan_grade, test_size=0.2, random_state=42)

"""## Resampling Imbalance Data

###Variabel Target Loan_status
"""

data_plot_y_loan = y_train_loan_status.reset_index().drop('index',axis = 1)
data_plot_y_loan = data_plot_y_loan.groupby('loan_status')['loan_status'].count().reset_index(name = 'count')
data_plot_y_loan

plot_loan_imbelance = px.bar(data_plot_y_loan, x = 'loan_status', y = 'count')


# Lakukan SMOTE pada subset training
# Lakukan Random Oversampling pada subset training
ros = RandomOverSampler(random_state=42)
X_train_resampled_loan, y_train_resampled_loan = ros.fit_resample(X_train, y_train_loan_status)
X_train_resampled_loan

data_plot_y_loan_resampled = y_train_resampled_loan.reset_index().drop('index', axis = 1)
data_plot_y_loan_resampled = data_plot_y_loan_resampled.groupby('loan_status')['loan_status'].count().reset_index(name = 'count')
data_plot_y_loan_resampled

"""###Variabel Target Grade"""

data_plot_y_grade = y_train_loan_grade.reset_index().drop('index',axis = 1)
data_plot_y_grade = data_plot_y_grade.groupby('loan_grade')['loan_grade'].count().reset_index(name = 'count')
data_plot_y_grade

plot_loan_imbelance = px.bar(data_plot_y_grade, x = 'loan_grade', y = 'count')


# Lakukan SMOTE pada subset training
# Lakukan Random Oversampling pada subset training
ros = RandomOverSampler(random_state=42)
X_train_resampled_grade, y_train_resampled_grade = ros.fit_resample(X_train, y_train_loan_grade)
X_train_resampled_grade

data_plot_y_grade_resampled = y_train_resampled_grade.reset_index().drop('index', axis = 1)
data_plot_y_grade_resampled = data_plot_y_grade_resampled.groupby('loan_grade')['loan_grade'].count().reset_index(name = 'count')
data_plot_y_grade_resampled

"""## Modelling

### Decision Tree
"""

param_grid = {'max_depth' :[10,20, 30,40,50, None],
              'max_features' : ['sqrt', 'log2', 'auto', None],
              'min_samples_split' : [2,5,10,12,15],
              'min_samples_leaf': [1,2,4,6,8,12],
              'splitter' :['best', 'random'],
              'criterion': ['gini', 'entropy', 'log_loss']}

dt = DecisionTreeClassifier()
grid = GridSearchCV(dt, param_grid=param_grid, cv = 5,n_jobs = -1)
grid.fit(X_train_resampled_loan, y_train_resampled_loan)

print("Best Parameters : \n" , grid.best_params_)
print("\nBest Estimator: \n", grid.best_estimator_)

y_pred_loan= grid.predict(X_test)
print("\n Classification Report: \n", classification_report(y_test_loan_status, y_pred_loan))
cm = confusion_matrix(y_test_loan_status, y_pred_loan)

sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


param_grid = {'max_depth' :[10,20, 30,40,50, None],
              'max_features' : ['sqrt', 'log2', 'auto', None],
              'min_samples_split' : [1,2,3,None],
              'min_samples_leaf': [1,2,3, None],
              'splitter' :['best', 'random'],
              'criterion': ['gini', 'entropy', 'log_loss']}

dt = DecisionTreeClassifier()
grid = GridSearchCV(dt, param_grid=param_grid, cv = 5,n_jobs = -1)
grid.fit(X_train_resampled_grade, y_train_resampled_grade)

print("Best Parameters : \n" , grid.best_params_)
print("\nBest Estimator: \n", grid.best_estimator_)

y_pred_grade= grid.predict(X_test)
print("\n Classification Report: \n", classification_report(y_test_loan_grade, y_pred_grade))
cm = confusion_matrix(y_test_loan_grade, y_pred_grade)

sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


from joblib import dump
dump(grid, 'Decision_tree_model_loan.pkl')

from joblib import dump
dump(grid, 'Decision_tree_model_grade.pkl')

"""## SVM"""

svc = SVC()

parameters = {'kernel': ['linear', 'rbf'], 
             'C': [0.1, 1, 10, 100], 
             'gamma': [0.01, 0.1, 1, 10],
              'degree' : [3,6,10,40,50],
              'max_iter' : [10,20,40,60,70,100],
              }


model_svm = GridSearchCV(svc, parameters)
model_svm.fit(X_train_resampled_loan, y_train_resampled_loan)


print('Best Parameters : \n', model_svm.best_params_)
print('\nBest Estimator : \n', model_svm.best_estimator_)

y_pred_loan = model_svm.predict(X_test)
print('\n Classification Report : \n', classification_report(y_test_loan_status, y_pred_loan))

cm = confusion_matrix(y_test_loan_status, y_pred_loan)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


dump(model_svm, 'SVM_model_grade.joblib')



"""## ANN"""

model_ann = Sequential()
model_ann.add(Dense(6, input_dim = X_train_resampled_loan.shape[1], activation = 'relu'))
model_ann.add(Dense(12, activation = 'relu'))
model_ann.add(Dense(1, activation = 'sigmoid'))

model_ann.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model_ann.fit(X_train_resampled_loan, y_train_resampled_loan, epochs = 100, batch_size = 64, validation_data = (X_test, y_test_loan_status))


y_pred_loan = model_ann.predict(X_test)
threshold = 0.5
y_pred_loan = (y_pred_loan > threshold).astype(int)


print('\n Classification Report : \n', classification_report(y_test_loan_status, y_pred_loan))
cm = confusion_matrix(y_test_loan_status, y_pred_loan)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


"""## KNN"""

knn = KNeighborsClassifier()
params = {
    'n_neighbors' : [1,2],
    'weights'     : ['uniform', 'distance'],
    'algorithm'   : ['brute', 'kd_tree', 'ball_tree'],
    'p'           : [2,3,4]
}

grid_search = GridSearchCV(knn, params, cv = 5, n_jobs = -1)

grid_search.fit(X_train_resampled_loan,y_train_resampled_loan)

accuracy = grid_search.score(X_test, y_test_loan_status)
print("Hyperparameter terbaik:", grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_loan = grid_search.predict(X_test)
print("Classification Report:\n", classification_report(y_test_loan_status, y_pred_loan))
cm = confusion_matrix(y_test_loan_status, y_pred_loan)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


from joblib import dump
dump(grid_search, 'KNN_model_loan_status.pkl')

knn = KNeighborsClassifier()
params = {
    'n_neighbors' : [1,2,5,6,7],
    'weights'     : ['uniform', 'distance'],
    'algorithm'   : ['brute', 'kd_tree', 'ball_tree'],
    'p'           : [2,3,4]
}

grid_search = GridSearchCV(knn, params, cv = 5, n_jobs = -1)

grid_search.fit(X_train_resampled_grade,y_train_resampled_grade)

accuracy = grid_search.score(X_test, y_test_loan_grade)
print("Hyperparameter terbaik:", grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_grade = grid_search.predict(X_test)
print("Classification Report:\n", classification_report(y_test_loan_grade, y_pred_grade))
cm = confusion_matrix(y_test_loan_grade, y_pred_grade)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


from joblib import dump
dump(grid_search, 'KNN_model_loan_grade.joblib')

"""## XGboost"""

import xgboost as xgb
xgb_model = xgb.XGBClassifier(random_state=42)

# Set hyperparameter yang akan di-tune
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.3, 0.5],
    'n_estimators': [50, 100, 200],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.5, 0.7, 1],
    'colsample_bytree': [0.5, 0.7, 1],
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [1, 2, 5]
}

# Tuning hyperparameter menggunakan GridSearchCV
grid_search = GridSearchCV(xgb_model, param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_resampled_loan, y_train_resampled_loan)

accuracy = grid_search.score(X_test, y_test_loan_status)
print("Hyperparameter terbaik:", grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_loan = grid_search.predict(X_test)
print("Classification Report:\n", classification_report(y_test_loan_status, y_pred_loan))
cm = confusion_matrix(y_test_loan_status, y_pred_loan)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


dump(grid_search, 'Xgboost_model_loan_status.joblib')

import xgboost as xgb
xgb_model = xgb.XGBClassifier(random_state=42)

# Set hyperparameter yang akan di-tune
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.3, 0.5],
    'n_estimators': [50, 100, 200],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.5, 0.7, 1],
    'colsample_bytree': [0.5, 0.7, 1],
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [1, 2, 5]
}

# Tuning hyperparameter menggunakan GridSearchCV
grid_search = GridSearchCV(xgb_model, param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_resampled_grade, y_train_resampled_grade)

accuracy = grid_search.score(X_test, y_test_loan_grade)
print("Hyperparameter terbaik:", grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_grade = grid_search.predict(X_test)
print("Classification Report:\n", classification_report(y_test_loan_grade, y_pred_grade))
cm = confusion_matrix(y_test_loan_grade, y_pred_grade)
sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')


dump(grid_search, 'Xgboost_model_loan_grade.joblib')

"""## Ramdom Forest"""

rf = RandomForestClassifier(random_state = 42)

param_grid = {
    'n_estimators'      : [50,100,200,400],
    'max_depth'         : [5,10,20,50],
    'min_samples_split' : [2,5,10],
    'min_samples_leaf'  : [1,2,4]
}

grid_search = GridSearchCV(rf, param_grid, cv = 5, n_jobs = 1, verbose =2)
grid_search.fit(X_train_resampled_loan, y_train_resampled_loan)

accuracy = grid_search.score(X_test, y_test_loan_status)
print("Best Parameters\n" )
print(grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_loan_status = grid_search.predict(X_test)
print("classification Report :\n", classification_report(y_test_loan_status, y_pred_loan_status))
cm = confusion_matrix(y_test_loan_status, y_pred_loan_status)
sns.heatmap(cm,annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')

dump(grid_search, 'Random_forest_model_loan_status.joblib')

rf = RandomForestClassifier(random_state = 42)

param_grid = {
    'n_estimators'      : [50,100,200,400],
    'max_depth'         : [5,10,20,50],
    'min_samples_split' : [2,5,10],
    'min_samples_leaf'  : [1,2,4]
}

grid_search = GridSearchCV(rf, param_grid, cv = 5, n_jobs = 1, verbose =2)
grid_search.fit(X_train_resampled_grade, y_train_resampled_grade)

accuracy = grid_search.score(X_test, y_test_loan_grade)
print("Best Parameters\n")
print(grid_search.best_params_)
print("Akurasi model KNN: {:.2f}%".format(accuracy * 100))

y_pred_loan_grade = grid_search.predict(X_test)
print("classification Report :\n", classification_report(y_test_loan_grade, y_pred_loan_grade))
cm = confusion_matrix(y_test_loan_grade, y_pred_loan_grade)
sns.heatmap(cm,annot = True, cmap = 'Blues', fmt = 'g')
plt.xlabel('Predicted')
plt.ylabel('Actual')

dump(grid_search, 'Random_forest_model_loan_grade.joblib')